{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6808e4-ddf9-42f9-9981-c417f6813a54",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa89655d-1329-4d1f-9a93-c085c0b61679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saugatmalla/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/saugatmalla/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Python basic Ml libraries\n",
    "import numpy as np # Numerical processing\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization \n",
    "import matplotlib.pyplot as plt # For visulization \n",
    "\n",
    "#ML frameworks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# DL frameworks\n",
    "import torch # Tensor computation\n",
    "import torch.nn as nn # Neural Networks\n",
    "import torch.optim as optim # Neural Nework optimizers\n",
    "import torchtext # Text processing\n",
    "import tqdm # For measuing progress\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Required additional libaries\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "import collections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbce74-83f1-4a75-af4c-d217bf8ce143",
   "metadata": {},
   "source": [
    "# Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd866fa5-40ca-4a3f-8fad-9a9408eff61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d783b010-9e9b-44ef-9cc2-b3f1050baadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_book_works.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_book_authors.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_book_series.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_books.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_book_genres_initial.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_children.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_comics_graphic.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_fantasy_paranormal.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_history_biography.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_mystery_thriller_crime.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_poetry.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_romance.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_books_young_adult.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_children.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_comics_graphic.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_fantasy_paranormal.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_history_biography.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_mystery_thriller_crime....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_poetry.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_romance.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_interactions_young_adult.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_children.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_comics_graphic.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_fantasy_paranormal.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_history_biography.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_mystery_thriller_crime.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_poetry.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_romance.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>byGenre</td>\n",
       "      <td>goodreads_reviews_young_adult.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>complete</td>\n",
       "      <td>book_id_map.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>complete</td>\n",
       "      <td>user_id_map.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_interactions.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_reviews_dedup.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_reviews_spoiler.json.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>complete</td>\n",
       "      <td>goodreads_reviews_spoiler_raw.json.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                               name\n",
       "0   complete                       goodreads_book_works.json.gz\n",
       "1   complete                     goodreads_book_authors.json.gz\n",
       "2   complete                      goodreads_book_series.json.gz\n",
       "3   complete                            goodreads_books.json.gz\n",
       "4   complete              goodreads_book_genres_initial.json.gz\n",
       "5    byGenre                   goodreads_books_children.json.gz\n",
       "6    byGenre             goodreads_books_comics_graphic.json.gz\n",
       "7    byGenre         goodreads_books_fantasy_paranormal.json.gz\n",
       "8    byGenre          goodreads_books_history_biography.json.gz\n",
       "9    byGenre     goodreads_books_mystery_thriller_crime.json.gz\n",
       "10   byGenre                     goodreads_books_poetry.json.gz\n",
       "11   byGenre                    goodreads_books_romance.json.gz\n",
       "12   byGenre                goodreads_books_young_adult.json.gz\n",
       "13   byGenre            goodreads_interactions_children.json.gz\n",
       "14   byGenre      goodreads_interactions_comics_graphic.json.gz\n",
       "15   byGenre  goodreads_interactions_fantasy_paranormal.json.gz\n",
       "16   byGenre   goodreads_interactions_history_biography.json.gz\n",
       "17   byGenre  goodreads_interactions_mystery_thriller_crime....\n",
       "18   byGenre              goodreads_interactions_poetry.json.gz\n",
       "19   byGenre             goodreads_interactions_romance.json.gz\n",
       "20   byGenre         goodreads_interactions_young_adult.json.gz\n",
       "21   byGenre                 goodreads_reviews_children.json.gz\n",
       "22   byGenre           goodreads_reviews_comics_graphic.json.gz\n",
       "23   byGenre       goodreads_reviews_fantasy_paranormal.json.gz\n",
       "24   byGenre        goodreads_reviews_history_biography.json.gz\n",
       "25   byGenre   goodreads_reviews_mystery_thriller_crime.json.gz\n",
       "26   byGenre                   goodreads_reviews_poetry.json.gz\n",
       "27   byGenre                  goodreads_reviews_romance.json.gz\n",
       "28   byGenre              goodreads_reviews_young_adult.json.gz\n",
       "29  complete                                    book_id_map.csv\n",
       "30  complete                                    user_id_map.csv\n",
       "31  complete                         goodreads_interactions.csv\n",
       "32  complete                    goodreads_reviews_dedup.json.gz\n",
       "33  complete                  goodreads_reviews_spoiler.json.gz\n",
       "34  complete              goodreads_reviews_spoiler_raw.json.gz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fileNames = pd.read_csv(os.path.join(DIR,\"dataset_names.csv\"))\n",
    "display(fileNames)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79238e6e-2d40-4ffd-b340-54ba0bfc0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing url for donwloading the dataset\n",
    "fileName_type_mapping = dict(zip(fileNames['name'].values, fileNames['type'].values))\n",
    "fileName_url_mapping = {}\n",
    "\n",
    "for fName in fileName_type_mapping:\n",
    "    ftype = fileName_type_mapping[fName]\n",
    "    if ftype == \"complete\":\n",
    "        url = 'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/'+fName\n",
    "        fileName_url_mapping[fName] = url\n",
    "    elif ftype == \"byGenre\":\n",
    "        url = 'https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/goodreads/byGenre/'+fName\n",
    "        fileName_url_mapping[fName] = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ba10d6-675b-4885-8dec-c407ca96258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_byName(fname, local_fileName):\n",
    "    if fname in fileName_url_mapping:\n",
    "        url = fileName_url_mapping[fname]\n",
    "\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            with open(local_fileName, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(\"Dataset \", fname, \" has been downloaded\")\n",
    "    else:\n",
    "        print(\"Dataset \", fname, \" cannot be found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9963ae32-06c7-4555-bd27-0a13c9a07b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading the dataset without GUI\n",
    "# outDir = './customerReviews'\n",
    "\n",
    "# if not os.path.exists(outDir):\n",
    "#     os.makedirs(outDir)\n",
    "    \n",
    "# output_path = os.path.join(outDir, 'goodreads_reviews_dedup.json.gz')\n",
    "# download_byName('goodreads_reviews_dedup.json.gz', output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9ba629-0428-44ea-82cf-eae03d0397d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a730630d-822a-4fca-b724-8158a8397198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287a548-ac36-4127-ac97-65ba0d8dfb28",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e3e90a-efcb-402a-b41f-f6cdc435b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "datasetPath = './customerReviews/goodreads_reviews_dedup.json.gz'\n",
    "\n",
    "chunk_size = 10000  # Adjust the chunk size based on your system's memory capacity\n",
    "\n",
    "# With pandas\n",
    "chunks = pd.read_json(datasetPath, lines=True, compression='gzip', chunksize=chunk_size, nrows=100000)\n",
    "# Read JSON data with Dask\n",
    "#ddf = dd.read_json(datasetPath, lines=True, compression='gzip').head(1000)\n",
    "\n",
    "# Generator expression to read random rows (200000 in this case) from each chunk\n",
    "sampled_chunks = (chunk.sample(n=10000, random_state=42) for chunk in chunks)\n",
    "# Sample the dataframe with dask\n",
    "#sampled_ddf = ddf.sample(frac=0.001, random_state=42).head(1)  # Adjust the fraction as needed\n",
    "\n",
    "\n",
    "# # Concatenate teh sampled chunks into a dataframe\n",
    "df = pd.concat(sampled_chunks, ignore_index=True)\n",
    "# Compute the Dask dataframe to obtain a Pandas dataframe\n",
    "#df = sampled_ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8915b79-c929-4099-a3e4-6a582674b443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d4b1dcb35db677f20ee45225a5e43be2</td>\n",
       "      <td>93436</td>\n",
       "      <td>d48b5afd9f57e588ac3c91dfe7fa859b</td>\n",
       "      <td>5</td>\n",
       "      <td>Amazing. I cannot believe it took me this long...</td>\n",
       "      <td>Sun Aug 05 15:22:23 -0700 2007</td>\n",
       "      <td>Thu Dec 17 03:51:57 -0800 2009</td>\n",
       "      <td>Sat Sep 01 00:00:00 -0700 2007</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd6522e9018f2f77332ec74f928f8c45</td>\n",
       "      <td>266765</td>\n",
       "      <td>0b9d92d61f295fcb08fd1e14761f52d4</td>\n",
       "      <td>3</td>\n",
       "      <td>Ihan hyva omassa kategoriassaan, mutta pakosti...</td>\n",
       "      <td>Sat Jul 28 02:33:08 -0700 2012</td>\n",
       "      <td>Sat Jul 28 02:35:03 -0700 2012</td>\n",
       "      <td>Wed Sep 13 00:00:00 -0700 2006</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0ef32090550901ead25cb0ea21c4d36b</td>\n",
       "      <td>35282714</td>\n",
       "      <td>9d12eb66d95d1c20eb8d2db1e1ee40b2</td>\n",
       "      <td>3</td>\n",
       "      <td>This is a solid follow up to The Girl on the T...</td>\n",
       "      <td>Tue Jun 20 07:48:07 -0700 2017</td>\n",
       "      <td>Tue Jun 20 07:56:12 -0700 2017</td>\n",
       "      <td>Mon Jun 19 00:00:00 -0700 2017</td>\n",
       "      <td>Fri Jun 02 00:00:00 -0700 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cd6522e9018f2f77332ec74f928f8c45</td>\n",
       "      <td>3049635</td>\n",
       "      <td>cdfb7a66f16f65a3f2fdf9d826c9c516</td>\n",
       "      <td>3</td>\n",
       "      <td>Muutama sana koko sarjasta. Sarja alkaa hyvin,...</td>\n",
       "      <td>Thu Jul 26 11:54:22 -0700 2012</td>\n",
       "      <td>Thu Jul 26 11:58:33 -0700 2012</td>\n",
       "      <td>Wed Apr 26 00:00:00 -0700 2006</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cd6522e9018f2f77332ec74f928f8c45</td>\n",
       "      <td>5560595</td>\n",
       "      <td>9eefa83d1150440cb240f3ec78ee9e78</td>\n",
       "      <td>2</td>\n",
       "      <td>Uusin Marklund, joka on tuttuun tapaan helppo ...</td>\n",
       "      <td>Thu Aug 16 08:26:00 -0700 2012</td>\n",
       "      <td>Thu Aug 16 08:30:54 -0700 2012</td>\n",
       "      <td>Wed Nov 05 00:00:00 -0800 2008</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>edc68f0f9e163d47b1503b3cdc4e2c5e</td>\n",
       "      <td>25667918</td>\n",
       "      <td>035fc283d2c09b015037ff5f5897d59d</td>\n",
       "      <td>4</td>\n",
       "      <td>This was so ferocious/heart-warming! \\n Fangir...</td>\n",
       "      <td>Mon Jun 08 14:24:35 -0700 2015</td>\n",
       "      <td>Tue Dec 20 21:57:47 -0800 2016</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>a6d2293f4d6542e2edb335eab2daad39</td>\n",
       "      <td>1857440</td>\n",
       "      <td>07aeb7de221c8f16656fb46f072c38bb</td>\n",
       "      <td>5</td>\n",
       "      <td>A highly entertaining, occasionally hilarious,...</td>\n",
       "      <td>Sun Jul 07 21:26:13 -0700 2013</td>\n",
       "      <td>Sun Jul 07 21:29:09 -0700 2013</td>\n",
       "      <td>Sun Jul 07 00:00:00 -0700 2013</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>a6d2293f4d6542e2edb335eab2daad39</td>\n",
       "      <td>220926</td>\n",
       "      <td>8ae4e5ec0cb9b49a2b8b0e6eaefb707c</td>\n",
       "      <td>3</td>\n",
       "      <td>The ideas in this book are as dense as the wor...</td>\n",
       "      <td>Sun May 27 22:48:25 -0700 2007</td>\n",
       "      <td>Sat Jul 14 06:21:38 -0700 2007</td>\n",
       "      <td>Sun Jul 01 00:00:00 -0700 2007</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>37b3e60b4e4152c580fd798d405150ff</td>\n",
       "      <td>5067355</td>\n",
       "      <td>d5f621f9f89aeb6e4c1f38d1671a9860</td>\n",
       "      <td>4</td>\n",
       "      <td>Great characters &amp; a story that continues to b...</td>\n",
       "      <td>Sat Aug 25 14:30:28 -0700 2012</td>\n",
       "      <td>Sat Aug 25 14:30:47 -0700 2012</td>\n",
       "      <td>Wed Dec 17 00:00:00 -0800 2008</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>780b52a9ec057a458031353110c276f7</td>\n",
       "      <td>102439</td>\n",
       "      <td>aeb78b1bf5bfd98952ed799b5c3cbe9a</td>\n",
       "      <td>4</td>\n",
       "      <td>A nice near-future story of personal transform...</td>\n",
       "      <td>Mon Apr 15 12:27:09 -0700 2013</td>\n",
       "      <td>Thu Apr 18 18:03:05 -0700 2013</td>\n",
       "      <td>Wed Apr 17 00:00:00 -0700 2013</td>\n",
       "      <td>Mon Apr 15 00:00:00 -0700 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id   book_id  \\\n",
       "0      d4b1dcb35db677f20ee45225a5e43be2     93436   \n",
       "1      cd6522e9018f2f77332ec74f928f8c45    266765   \n",
       "2      0ef32090550901ead25cb0ea21c4d36b  35282714   \n",
       "3      cd6522e9018f2f77332ec74f928f8c45   3049635   \n",
       "4      cd6522e9018f2f77332ec74f928f8c45   5560595   \n",
       "...                                 ...       ...   \n",
       "99995  edc68f0f9e163d47b1503b3cdc4e2c5e  25667918   \n",
       "99996  a6d2293f4d6542e2edb335eab2daad39   1857440   \n",
       "99997  a6d2293f4d6542e2edb335eab2daad39    220926   \n",
       "99998  37b3e60b4e4152c580fd798d405150ff   5067355   \n",
       "99999  780b52a9ec057a458031353110c276f7    102439   \n",
       "\n",
       "                              review_id  rating  \\\n",
       "0      d48b5afd9f57e588ac3c91dfe7fa859b       5   \n",
       "1      0b9d92d61f295fcb08fd1e14761f52d4       3   \n",
       "2      9d12eb66d95d1c20eb8d2db1e1ee40b2       3   \n",
       "3      cdfb7a66f16f65a3f2fdf9d826c9c516       3   \n",
       "4      9eefa83d1150440cb240f3ec78ee9e78       2   \n",
       "...                                 ...     ...   \n",
       "99995  035fc283d2c09b015037ff5f5897d59d       4   \n",
       "99996  07aeb7de221c8f16656fb46f072c38bb       5   \n",
       "99997  8ae4e5ec0cb9b49a2b8b0e6eaefb707c       3   \n",
       "99998  d5f621f9f89aeb6e4c1f38d1671a9860       4   \n",
       "99999  aeb78b1bf5bfd98952ed799b5c3cbe9a       4   \n",
       "\n",
       "                                             review_text  \\\n",
       "0      Amazing. I cannot believe it took me this long...   \n",
       "1      Ihan hyva omassa kategoriassaan, mutta pakosti...   \n",
       "2      This is a solid follow up to The Girl on the T...   \n",
       "3      Muutama sana koko sarjasta. Sarja alkaa hyvin,...   \n",
       "4      Uusin Marklund, joka on tuttuun tapaan helppo ...   \n",
       "...                                                  ...   \n",
       "99995  This was so ferocious/heart-warming! \\n Fangir...   \n",
       "99996  A highly entertaining, occasionally hilarious,...   \n",
       "99997  The ideas in this book are as dense as the wor...   \n",
       "99998  Great characters & a story that continues to b...   \n",
       "99999  A nice near-future story of personal transform...   \n",
       "\n",
       "                           date_added                    date_updated  \\\n",
       "0      Sun Aug 05 15:22:23 -0700 2007  Thu Dec 17 03:51:57 -0800 2009   \n",
       "1      Sat Jul 28 02:33:08 -0700 2012  Sat Jul 28 02:35:03 -0700 2012   \n",
       "2      Tue Jun 20 07:48:07 -0700 2017  Tue Jun 20 07:56:12 -0700 2017   \n",
       "3      Thu Jul 26 11:54:22 -0700 2012  Thu Jul 26 11:58:33 -0700 2012   \n",
       "4      Thu Aug 16 08:26:00 -0700 2012  Thu Aug 16 08:30:54 -0700 2012   \n",
       "...                               ...                             ...   \n",
       "99995  Mon Jun 08 14:24:35 -0700 2015  Tue Dec 20 21:57:47 -0800 2016   \n",
       "99996  Sun Jul 07 21:26:13 -0700 2013  Sun Jul 07 21:29:09 -0700 2013   \n",
       "99997  Sun May 27 22:48:25 -0700 2007  Sat Jul 14 06:21:38 -0700 2007   \n",
       "99998  Sat Aug 25 14:30:28 -0700 2012  Sat Aug 25 14:30:47 -0700 2012   \n",
       "99999  Mon Apr 15 12:27:09 -0700 2013  Thu Apr 18 18:03:05 -0700 2013   \n",
       "\n",
       "                              read_at                      started_at  \\\n",
       "0      Sat Sep 01 00:00:00 -0700 2007                                   \n",
       "1      Wed Sep 13 00:00:00 -0700 2006                                   \n",
       "2      Mon Jun 19 00:00:00 -0700 2017  Fri Jun 02 00:00:00 -0700 2017   \n",
       "3      Wed Apr 26 00:00:00 -0700 2006                                   \n",
       "4      Wed Nov 05 00:00:00 -0800 2008                                   \n",
       "...                               ...                             ...   \n",
       "99995                                                                   \n",
       "99996  Sun Jul 07 00:00:00 -0700 2013                                   \n",
       "99997  Sun Jul 01 00:00:00 -0700 2007                                   \n",
       "99998  Wed Dec 17 00:00:00 -0800 2008                                   \n",
       "99999  Wed Apr 17 00:00:00 -0700 2013  Mon Apr 15 00:00:00 -0700 2013   \n",
       "\n",
       "       n_votes  n_comments  \n",
       "0            0           0  \n",
       "1            0           0  \n",
       "2            1           0  \n",
       "3            0           0  \n",
       "4            0           0  \n",
       "...        ...         ...  \n",
       "99995        7           0  \n",
       "99996        0           0  \n",
       "99997        0           4  \n",
       "99998        0           0  \n",
       "99999        0           0  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b287f3-11e7-4eb3-9fb6-8860f6d44b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reviews(file_name):\n",
    "    print('counting file:', file_name)\n",
    "    n_review = 0\n",
    "    book_set, user_set = set(), set()\n",
    "    print('current line: ', end='')\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            if n_review % 1000000 == 0:\n",
    "                print(n_review, end=',')\n",
    "            n_review += 1\n",
    "            book_set.add(d['book_id'])\n",
    "            user_set.add(d['user_id'])\n",
    "    print('complete')\n",
    "    print('done!')\n",
    "    return n_review, len(book_set), len(user_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c8cff1-8b5a-4d3b-9b4a-d4a340835351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting file: ./customerReviews/goodreads_reviews_dedup.json.gz\n",
      "current line: 0,1000000,2000000,3000000,4000000,5000000,6000000,7000000,8000000,9000000,10000000,11000000,12000000,13000000,14000000,15000000,complete\n",
      "done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th># review</th>\n",
       "      <td>15739967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># book</th>\n",
       "      <td>2080190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># user</th>\n",
       "      <td>465323.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count\n",
       "# review  15739967.0\n",
       "# book     2080190.0\n",
       "# user      465323.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DIR = './customerReviews'\n",
    "n_review, n_book, n_user = count_reviews(os.path.join(DIR, 'goodreads_reviews_dedup.json.gz'))\n",
    "df_stats_review = pd.DataFrame([n_review, n_book, n_user], dtype=float,\n",
    "                               columns=['count'], index=['# review', '# book', '# user'])\n",
    "display(df_stats_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0371754-d239-4316-8f68-91ea6a9c4935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th># review</th>\n",
       "      <td>15739967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># book</th>\n",
       "      <td>2080190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># user</th>\n",
       "      <td>465323.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count\n",
       "# review  15739967.0\n",
       "# book     2080190.0\n",
       "# user      465323.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee832610-430a-496c-b6a6-7a633526be43",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bed91-b7d7-4c13-bdba-421dfce473f6",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45164062-3a68-4f4d-abd8-17673b5ab9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['book_id', 'rating', 'review_text']\n",
    "\n",
    "x_data = df[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23859e46-2b61-4018-9a94-d32353a30e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93436</td>\n",
       "      <td>5</td>\n",
       "      <td>Amazing. I cannot believe it took me this long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266765</td>\n",
       "      <td>3</td>\n",
       "      <td>Ihan hyva omassa kategoriassaan, mutta pakosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35282714</td>\n",
       "      <td>3</td>\n",
       "      <td>This is a solid follow up to The Girl on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3049635</td>\n",
       "      <td>3</td>\n",
       "      <td>Muutama sana koko sarjasta. Sarja alkaa hyvin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5560595</td>\n",
       "      <td>2</td>\n",
       "      <td>Uusin Marklund, joka on tuttuun tapaan helppo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>25667918</td>\n",
       "      <td>4</td>\n",
       "      <td>This was so ferocious/heart-warming! \\n Fangir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1857440</td>\n",
       "      <td>5</td>\n",
       "      <td>A highly entertaining, occasionally hilarious,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>220926</td>\n",
       "      <td>3</td>\n",
       "      <td>The ideas in this book are as dense as the wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>5067355</td>\n",
       "      <td>4</td>\n",
       "      <td>Great characters &amp; a story that continues to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>102439</td>\n",
       "      <td>4</td>\n",
       "      <td>A nice near-future story of personal transform...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        book_id  rating                                        review_text\n",
       "0         93436       5  Amazing. I cannot believe it took me this long...\n",
       "1        266765       3  Ihan hyva omassa kategoriassaan, mutta pakosti...\n",
       "2      35282714       3  This is a solid follow up to The Girl on the T...\n",
       "3       3049635       3  Muutama sana koko sarjasta. Sarja alkaa hyvin,...\n",
       "4       5560595       2  Uusin Marklund, joka on tuttuun tapaan helppo ...\n",
       "...         ...     ...                                                ...\n",
       "99995  25667918       4  This was so ferocious/heart-warming! \\n Fangir...\n",
       "99996   1857440       5  A highly entertaining, occasionally hilarious,...\n",
       "99997    220926       3  The ideas in this book are as dense as the wor...\n",
       "99998   5067355       4  Great characters & a story that continues to b...\n",
       "99999    102439       4  A nice near-future story of personal transform...\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8652ae8a-adb4-4d9d-be88-9c07a6bf4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seeds to ensure that result is reproducable nad we get the same results everytime the notebook is run.\n",
    "seed = 1234\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899e0f1-0bad-4d24-a5ac-7e9e59a63770",
   "metadata": {},
   "source": [
    "### Creating custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd650e26-bb35-4c35-ac89-ead4f3cd4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Another way:\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, dataframe, transform=None):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.dataframe.iloc[idx].to_dict()\n",
    "\n",
    "#         # Apply any preprocessing or transformations if needed\n",
    "#         if self.transform:\n",
    "#             sample['review_text'] = self.transform(sample['review_text'])\n",
    "\n",
    "#         return sample\n",
    "\n",
    "# # Preprocessing for text data using scikit learn\n",
    "# text_pipeline = Pipeline([\n",
    "#     ('vect', CountVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer())\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Create instances of the dataset\n",
    "# train_dataset = CustomDataset(train_data, transform=text_pipeline)\n",
    "# val_dataset = CustomDataset(val_data, transform=text_pipeline)\n",
    "# test_dataset = CustomDataset(test_data, transform=text_pipeline)\n",
    "\n",
    "# # Create PyTorch DataLoaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b48027-f078-46ee-9362-d1f8f61d1bf5",
   "metadata": {},
   "source": [
    "## For Getting labels \n",
    "#### 0-2: Negative\n",
    "#### 3: Neutral\n",
    "#### 4-5: Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7020e4-0f43-4b13-93c7-50a88f5abb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation sets and test sets\n",
    "train_data, temp_data = train_test_split(x_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52faed30-0277-4c6c-8582-f9cb11bcc766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ihan hyva omassa kategoriassaan, mutta pakosti tuli verrattua Louise Rennisonin Georgia Nicholson-kirjoihin, jotka ovat tyyliltaan hyvin samankaltaisia, mutta parempia. Ei tamakaan huono ollut, mutta Georgiat ovat parempia.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['review_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8376e48-a715-4d22-b495-c8fd621af342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with review and its book_id\n",
    "train_data_br = []\n",
    "test_data_br = []\n",
    "valid_data_br = []\n",
    "\n",
    "# for r_data, b_data in zip(train_data['review_text'],train_data['book_id']):\n",
    "#     #print(train_data[data])\n",
    "#     train_data_br.append({'review':r_data, 'book_id':b_data})\n",
    "\n",
    "# for r_data, b_data in zip(test_data['review_text'],test_data['book_id']):\n",
    "#     test_data_br.append({'review':r_data, 'book_id':b_data})\n",
    "\n",
    "# for r_data, b_data in zip(val_data['review_text'],val_data['book_id']):\n",
    "#     valid_data_br.append({'review':r_data, 'book_id':b_data})\n",
    "\n",
    "for r_data, ra_data in zip(train_data['review_text'],train_data['rating']):\n",
    "    #print(train_data[data])\n",
    "    if(ra_data>3):\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    \n",
    "    train_data_br.append({'review':r_data, 'rating':label})\n",
    "\n",
    "for r_data, ra_data in zip(test_data['review_text'],test_data['rating']):\n",
    "    if(ra_data>3):\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    \n",
    "    test_data_br.append({'review':r_data, 'rating':label})\n",
    "\n",
    "for r_data, ra_data in zip(val_data['review_text'],val_data['rating']):\n",
    "    if(ra_data>3):\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    \n",
    "    valid_data_br.append({'review':r_data, 'rating':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1509949-cb44-40f1-ae96-0a9b1f2e3a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"Quick read, interesting background stories. Not sure I think even Graff is that crafty, plus you'd think JP would recognize someone who played such a major role in his life...\",\n",
       " 'rating': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9fc066d-22c7-4511-be72-2909d83d26dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quick read, interesting background stories. Not sure I think even Graff is that crafty, plus you'd think JP would recognize someone who played such a major role in his life...\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f3aa0d0-13fe-405a-b9fb-dfb876b475b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d876c1-c1c6-43d9-8351-2d8ddbec1fb7",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ad52166-6ff9-4384-a323-bd71e8cd9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc251e5-86ea-48f8-bbcb-ff972d8d60a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['goodreads', 'book', 'reviews', 'sentiment', 'analysis']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Goodreads book reviews sentiment analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f181a6aa-e868-4250-88fb-dc3312a03f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data after creating the dataset and dataloader\n",
    "\n",
    "# def tokenize_dataset(dataset, tokenizer, max_len):\n",
    "#     def tokenize_example(example):\n",
    "#         # Tokenize the review text using the tokenizer\n",
    "#         tokenized_inputs = tokenizer(\n",
    "#             example['review_text']\n",
    "#         )\n",
    "\n",
    "#         # Include other features if needed\n",
    "#         #tokenized_inputs['book_id'] = example['book_id']\n",
    "#         #tokenized_input['rating'] = example['rating']\n",
    "\n",
    "#         return tokenized_inputs\n",
    "#     return dataset.dataframe.apply(tokenize_example, axis=1)\n",
    "\n",
    "def tokenize_example(example, tokenizer, max_length):\n",
    "        # Tokenize the review text using the tokenizer\n",
    "        tokens = tokenizer(example[\"review\"])[:max_length]\n",
    "\n",
    "        # Include other features if needed\n",
    "        #tokenized_inputs['book_id'] = example['book_id']\n",
    "        #tokenized_input['rating'] = example['rating']\n",
    "\n",
    "        #return {\"tokens\":tokens}\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41db8c4b-afd9-4062-807e-6ba8ac7097d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 256\n",
    "\n",
    "# # Tokenize the datasets\n",
    "# tokenized_train_data = tokenize_dataset(train_dataset, tokenizer, max_length)\n",
    "# tokenized_test_data = tokenize_dataset(test_dataset, tokenizer, max_length)\n",
    "\n",
    "# # Update the datasets with tokenized data\n",
    "# train_dataset.dataframe = tokenized_train_data\n",
    "# test_dataset.dataframe = tokenized_test_data\n",
    "\n",
    "# Updating the dataloaders as well\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5ec06f1-62b8-41b7-ab72-f637b4402546",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "for index in range(len(train_data_br)):\n",
    "    tokens = tokenize_example(train_data_br[index], tokenizer, max_length)\n",
    "    \n",
    "    train_data_br[index][\"tokens\"] = tokens\n",
    "\n",
    "for index in range(len(test_data_br)):\n",
    "    tokens = tokenize_example(test_data_br[index], tokenizer, max_length)\n",
    "    \n",
    "    test_data_br[index][\"tokens\"] = tokens\n",
    "\n",
    "for index in range(len(valid_data_br)):\n",
    "    tokens = tokenize_example(valid_data_br[index], tokenizer, max_length)\n",
    "    \n",
    "    valid_data_br[index][\"tokens\"] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49f5c227-cb9f-4e6c-8bc8-bcf02ec74513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quick read, interesting background stories. Not sure I think even Graff is that crafty, plus you'd think JP would recognize someone who played such a major role in his life...\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "743347c5-e09a-4c5e-8d89-43fd04676df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"Quick read, interesting background stories. Not sure I think even Graff is that crafty, plus you'd think JP would recognize someone who played such a major role in his life...\",\n",
       " 'rating': 1,\n",
       " 'tokens': ['quick',\n",
       "  'read',\n",
       "  ',',\n",
       "  'interesting',\n",
       "  'background',\n",
       "  'stories',\n",
       "  '.',\n",
       "  'not',\n",
       "  'sure',\n",
       "  'i',\n",
       "  'think',\n",
       "  'even',\n",
       "  'graff',\n",
       "  'is',\n",
       "  'that',\n",
       "  'crafty',\n",
       "  ',',\n",
       "  'plus',\n",
       "  'you',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'think',\n",
       "  'jp',\n",
       "  'would',\n",
       "  'recognize',\n",
       "  'someone',\n",
       "  'who',\n",
       "  'played',\n",
       "  'such',\n",
       "  'a',\n",
       "  'major',\n",
       "  'role',\n",
       "  'in',\n",
       "  'his',\n",
       "  'life',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b0f90e9-ef78-45ed-a869-153825f54464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick',\n",
       " 'read',\n",
       " ',',\n",
       " 'interesting',\n",
       " 'background',\n",
       " 'stories',\n",
       " '.',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'i',\n",
       " 'think',\n",
       " 'even',\n",
       " 'graff',\n",
       " 'is',\n",
       " 'that',\n",
       " 'crafty',\n",
       " ',',\n",
       " 'plus',\n",
       " 'you',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'think',\n",
       " 'jp',\n",
       " 'would',\n",
       " 'recognize',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'played',\n",
       " 'such',\n",
       " 'a',\n",
       " 'major',\n",
       " 'role',\n",
       " 'in',\n",
       " 'his',\n",
       " 'life',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7269840a-0aa5-40cb-bfb6-09da26185b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick',\n",
       " 'read',\n",
       " ',',\n",
       " 'interesting',\n",
       " 'background',\n",
       " 'stories',\n",
       " '.',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'i',\n",
       " 'think',\n",
       " 'even',\n",
       " 'graff',\n",
       " 'is',\n",
       " 'that',\n",
       " 'crafty',\n",
       " ',',\n",
       " 'plus',\n",
       " 'you',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'think',\n",
       " 'jp',\n",
       " 'would',\n",
       " 'recognize']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0][\"tokens\"][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a41a3-fc41-4835-8bac-c5a0c5d849f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "363cb688-9cd3-427e-808d-ad27dd9519c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 10000, 10000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_br), len(test_data_br), len(valid_data_br)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688355b0-4796-4e6c-af89-1ca7801d5d5b",
   "metadata": {},
   "source": [
    "## Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4cd64ec-9cd2-4bba-8b88-7993403c4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    [train_data_br[index]['tokens'] for index in range(len(train_data_br))],\n",
    "    min_freq=min_freq, \n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c63473e-2771-4ccc-8cd8-53abced6262b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37969"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48c33fb0-8da8-4618-b7f8-a0db5395f55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '.', 'the', ',', 'and', 'i', 'a', 'to', \"'\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the tokens in the vocabulary\n",
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80399305-2ba4-4ebe-b794-3b7434239a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "860010d9-f537-47f1-a15b-c793ee726dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the index of unknown and padding special tokens\n",
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51f802b5-4c1d-4f2c-bf99-25f49c85800c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheking if token in vocab\n",
    "\"awesome\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3f6038c-c0b4-4cdc-8b83-4a2d77b80816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To overwrite the default error taht we get with torchtext when token not in vocab\n",
    "\n",
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fc25790-3bf3-4c69-b2ce-814f1a134c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['bike pencil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1a6a62f-5aa6-4d4d-b301-296e5b944044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5644, 121, 0, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To look at the list of tokens\n",
    "vocab.lookup_indices([\"hello\", \"world\", \"bike pencil\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662edfb9-0f36-493d-9ea0-b62fc1746012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46b85556-15b6-4aa0-82d1-354d829f852d",
   "metadata": {},
   "source": [
    "## Numericalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41d042e9-7e5a-461d-a705-04fe6b092bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get ids of tokens from that example\n",
    "def numericalize_example(example, vocab):\n",
    "    ids = vocab.lookup_indices(example[\"tokens\"])\n",
    "    #return {\"ids\": ids}\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11428583-2563-4685-8729-2c279587b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "for index in range(len(train_data_br)):\n",
    "    ids = numericalize_example(train_data_br[index], vocab)\n",
    "    \n",
    "    train_data_br[index][\"ids\"] = ids\n",
    "\n",
    "for index in range(len(test_data_br)):\n",
    "    ids = numericalize_example(test_data_br[index], vocab)\n",
    "    \n",
    "    test_data_br[index][\"ids\"] = ids\n",
    "\n",
    "for index in range(len(valid_data_br)):\n",
    "    ids = numericalize_example(valid_data_br[index], vocab)\n",
    "    \n",
    "    valid_data_br[index][\"ids\"] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73858698-2cce-4c0b-8554-18b2f5c1ba92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick',\n",
       " 'read',\n",
       " ',',\n",
       " 'interesting',\n",
       " 'background',\n",
       " 'stories',\n",
       " '.',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'i']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32b2f4ab-aa7a-4139-9c1d-494d602d218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[371, 26, 4, 122, 805, 156, 2, 28, 201, 6]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ids of token from that example\n",
    "vocab.lookup_indices(train_data_br[0][\"tokens\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72cc9e49-2840-4edb-a4cd-c4b4665e59d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[371, 26, 4, 122, 805, 156, 2, 28, 201, 6]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0][\"ids\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "876af61f-0ea4-4ce9-ba28-6d10e18410e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting ids and labels from integers into Pytorch tensors\n",
    "#train_data_br = train_data_br.with_format(type=\"torch\", columns=['ids', 'label'])\n",
    "\n",
    "for i in range(len(train_data_br)):\n",
    "    \n",
    "    # Convert ids and book_ids to PyTorch tensor\n",
    "    train_data_br[i]['ids'] = torch.tensor(train_data_br[i]['ids'])\n",
    "    train_data_br[i]['rating'] = torch.tensor(train_data_br[i]['rating'])\n",
    "\n",
    "    # Replace the existing 'ids' and 'book_id' values with PyTorch tensors\n",
    "    # train_data_br[i]['ids'] = ids_tensor.tolist() # If need to convert tensor back to lost\n",
    "    # train_data_br[i]['book_id'] = bookids_tensor.item()  # Convert tensor back to scalar\n",
    "\n",
    "for i in range(len(test_data_br)):\n",
    "    \n",
    "    # Convert ids and labels to PyTorch tensor\n",
    "    test_data_br[i]['ids'] = torch.tensor(test_data_br[i]['ids'])\n",
    "    test_data_br[i]['rating'] = torch.tensor(test_data_br[i]['rating'])\n",
    "\n",
    "for i in range(len(valid_data_br)):\n",
    "    \n",
    "    # Convert ids and labels to PyTorch tensor\n",
    "    valid_data_br[i]['ids'] = torch.tensor(valid_data_br[i]['ids'])\n",
    "    valid_data_br[i]['rating'] = torch.tensor(valid_data_br[i]['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a35c91c2-e80d-4be1-8d48-76ee2697e5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[1]['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c435fc59-fd9c-4fe6-a999-8833811a43b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([371,  26,   4, 122, 805, 156,   2,  28, 201,   6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0][\"ids\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "366ff70a-ad12-471e-9776-4796a8797688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick',\n",
       " 'read',\n",
       " ',',\n",
       " 'interesting',\n",
       " 'background',\n",
       " 'stories',\n",
       " '.',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'i']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the human redable token back\n",
    "vocab.lookup_tokens(train_data_br[0][\"ids\"][:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28441b-a66a-4a7d-9de4-4e85364d6ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9ddf066-8c19-47ca-a128-e1f20b2ac741",
   "metadata": {},
   "source": [
    "## Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d20614c3-dfb1-47e9-9f04-a66e4d329590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collate a batch\n",
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [i[\"ids\"] for i in batch]\n",
    "        batch_ids = nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        batch_rating = [i[\"rating\"] for i in batch]\n",
    "        batch_rating = torch.stack(batch_rating)\n",
    "        batch = {\"ids\": batch_ids, \"rating\": batch_rating}\n",
    "\n",
    "        return batch\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3ec4115-49c9-4462-8af4-cce3688365ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns actuial data loader.\n",
    "# batch size: (number of sentences in the batch), padding token index, shuffle dataset\n",
    "\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset, \n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cb7e7cb-6db7-48a5-8063-8d52e895d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data loader\n",
    "batch_size = 512\n",
    "\n",
    "train_data_loader = get_data_loader(train_data_br, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data_br, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data_br, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79bea1be-20fa-4556-b6a2-803e6adf1ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2e46b8b20>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c93003-7d6c-4d81-b10d-e38a0b012be7",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e997e2-ba60-4341-a356-f22642c6a65e",
   "metadata": {},
   "source": [
    "### Neural Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93867d2e-6d55-4e1a-a910-81bbb1ec1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBoW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        # ids = [batch size, seq len]\n",
    "        embedded = self.embedding(ids)\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        # pooled = [batch size, embeddiong dim]\n",
    "        prediction = self.fc(pooled)\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ee687bc-f0c5-4d91-8baf-7f90e4670e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[1]['rating'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14d4c0ea-106c-4ab1-9c8b-52ccf29bcf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unique(feature):\n",
    "#     uniqueList = []\n",
    "    \n",
    "#     for x in range(len(train_data_br)):\n",
    "#         if(train_data_br[x]['book_id'].tolist() not in uniqueList):\n",
    "#             uniqueList.append(train_data_br[x]['book_id'])\n",
    "#     return uniqueList\n",
    "\n",
    "def unique(feature):\n",
    "    \n",
    "    # uniqueList = {}\n",
    "    # for x in range(len(train_data_br)):\n",
    "    #     if(train_data_br[x]['rating'].tolist() not in uniqueList):\n",
    "    #         uniqueList[train_data_br[x]['rating']] = 1\n",
    "    uniqueList = []\n",
    "    for x in range(len(train_data_br)):\n",
    "        if(train_data_br[x]['rating'].tolist() not in uniqueList):\n",
    "            uniqueList.append(train_data_br[x]['rating'])\n",
    "    return uniqueList\n",
    "\n",
    "\n",
    "# We could have just counted the tensors as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56d5622d-cb21-452b-bffa-f8a49ddedd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "output_dim = len(unique(\"rating\"))\n",
    "\n",
    "model = NBoW(vocab_size, embedding_dim, output_dim, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5f133c2-fff0-42ac-b73e-d4d4f4c02e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea0f7891-a9b7-4071-91f9-a926a59b796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11,391,302 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6882d704-d75d-4fcb-b77b-8c52827db77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efa73398-eb56-4d9d-a17b-af9718170216",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_vector = vectors.get_vecs_by_tokens(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7c33408-176f-4311-9474-049a46426345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0675e9ae-04d6-4e23-ae24-d4637a1e919c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2523,  0.1018, -0.6748,  0.2112,  0.4349,  0.1654,  0.4826, -0.8122,\n",
       "         0.0413,  0.7850, -0.0779, -0.6632,  0.1464, -0.2929, -0.2549,  0.0193,\n",
       "        -0.2026,  0.9823,  0.0283, -0.0813, -0.1214,  0.1313, -0.1765,  0.1356,\n",
       "        -0.1636, -0.2257,  0.0550, -0.2031,  0.2072,  0.0958,  0.2248,  0.2154])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_vector[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77eb2943-28af-4777-a80f-a3276f00b8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37969, 300])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
    "pretrained_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c5753c0-8597-4276-bd6b-7d5d18c662dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -0.5592, -0.4480, -0.6476],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.7882, -1.6625, -0.7064,  ..., -1.5841, -0.3711, -1.2338],\n",
       "        ...,\n",
       "        [ 1.4103,  0.5182, -0.7226,  ...,  0.0552,  0.2366, -1.0831],\n",
       "        [-0.1828,  0.2844,  1.7399,  ..., -0.7286, -0.1993,  0.7478],\n",
       "        [-0.2539,  0.9267, -0.6750,  ...,  0.0374,  0.4367,  0.7049]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f36a5fdb-8a13-4f5b-8adf-3fc47248341f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79cdf755-a873-4990-bef5-129599ff643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e53a7879-0dc0-428a-b6e2-d38cc32cc7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df51c68d-1191-44f4-a419-72f9e9a4bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1331d38-e680-4b01-9291-55641136f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "576d4019-9590-4b03-a910-e092eb89fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in tqdm.tqdm(train_data_loader, desc=\"Checking\"):\n",
    "#     print(batch['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78f473e3-9689-4fe5-b931-8edf32040a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"Quick read, interesting background stories. Not sure I think even Graff is that crafty, plus you'd think JP would recognize someone who played such a major role in his life...\",\n",
       " 'rating': tensor(1),\n",
       " 'tokens': ['quick',\n",
       "  'read',\n",
       "  ',',\n",
       "  'interesting',\n",
       "  'background',\n",
       "  'stories',\n",
       "  '.',\n",
       "  'not',\n",
       "  'sure',\n",
       "  'i',\n",
       "  'think',\n",
       "  'even',\n",
       "  'graff',\n",
       "  'is',\n",
       "  'that',\n",
       "  'crafty',\n",
       "  ',',\n",
       "  'plus',\n",
       "  'you',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'think',\n",
       "  'jp',\n",
       "  'would',\n",
       "  'recognize',\n",
       "  'someone',\n",
       "  'who',\n",
       "  'played',\n",
       "  'such',\n",
       "  'a',\n",
       "  'major',\n",
       "  'role',\n",
       "  'in',\n",
       "  'his',\n",
       "  'life',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.'],\n",
       " 'ids': tensor([  371,    26,     4,   122,   805,   156,     2,    28,   201,     6,\n",
       "           104,   101, 35216,    12,    15, 17472,     4,   988,    32,     9,\n",
       "           206,   104, 18501,    75,  3155,   323,    57,  1580,   197,     7,\n",
       "           775,  1006,    13,    34,    90,     2,     2,     2])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_br[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38a13fe7-440b-4ed5-a2b5-c29577d1d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "# criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9d3bab7-1f4a-4ed3-8f7d-56a5a34501ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for batch in tqdm.tqdm(data_loader, desc=\"training...\"):\n",
    "        # ids = batch[\"ids\"].to(device)\n",
    "        # rating = batch[\"rating\"].to(device)\n",
    "        ids = batch[\"ids\"]\n",
    "        rating = batch[\"rating\"]\n",
    "        #print(\"Rating:\", rating)\n",
    "        \n",
    "        prediction = model(ids)\n",
    "        loss = criterion(prediction, rating)\n",
    "        accuracy = get_accuracy(prediction, rating)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c9bde6e-ed2b-4720-9129-a979e9014d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, criterion):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(data_loader, desc=\"evaluating...\"):\n",
    "            # ids = batch[\"ids\"].to(device)\n",
    "            # rating = batch[\"rating\"].to(device)\n",
    "            ids = batch[\"ids\"]\n",
    "            rating = batch[\"rating\"]\n",
    "            #print(\"Rating:\", rating)\n",
    "\n",
    "            prediction = model(ids)\n",
    "            loss = criterion(prediction, rating)\n",
    "            accuracy = get_accuracy(prediction, rating)\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "099fa109-eb92-44bf-8bb9-088d1797e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, rating):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    \n",
    "    correct_predictions = predicted_classes.eq(rating).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9659a3e5-5b6f-4948-ab57-7539ab43fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|████████████████████████████| 157/157 [00:03<00:00, 48.43it/s]\n",
      "evaluating...: 100%|████████████████████████████| 20/20 [00:00<00:00, 55.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train_loss: 0.699, train_acc: 0.414\n",
      "valid_loss: 0.699, valid_acc: 0.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:  25%|███████▏                     | 39/157 [00:00<00:02, 47.53it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m metrics \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 7\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m evaluate(valid_data_loader, model, criterion)\n\u001b[1;32m     11\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[76], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m rating \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#print(\"Rating:\", rating)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(prediction, rating)\n\u001b[1;32m     15\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(prediction, rating)\n",
      "File \u001b[0;32m~/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m, in \u001b[0;36mNBoW.forward\u001b[0;34m(self, ids)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# ids = [batch size, seq len]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# embedded = [batch size, seq len, embedding dim]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     pooled \u001b[38;5;241m=\u001b[39m embedded\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/LazyReader/py/lib/python3.9/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(\n",
    "        train_data_loader, model, criterion, optimizer\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate(valid_data_loader, model, criterion)\n",
    "    metrics[\"train_losses\"].append(train_loss)\n",
    "    metrics[\"train_accs\"].append(train_acc)\n",
    "    metrics[\"valid_losses\"].append(valid_loss)\n",
    "    metrics[\"valid_accs\"].append(valid_acc)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"nbow.pt\")\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "    print(f\"valid_loss: {valid_loss:.3f}, valid_acc: {valid_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395d289-6afa-4d28-bee5-05f039507946",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_losses\"], label=\"train loss\")\n",
    "ax.plot(metrics[\"valid_losses\"], label=\"valid loss\")\n",
    "\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ed91e-375e-407b-be7e-73f01f18c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_accs\"], label=\"train accuracy\")\n",
    "ax.plot(metrics[\"valid_accs\"], label=\"valid accuracy\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba955b-eec3-4f80-9850-df68298b684a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ba941-c72f-465b-b015-d2927041b48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7210af1-7495-40f3-9bc5-b9318503f65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ee5e4-36d7-4c9c-8624-9e55aa9a0868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6a9f4-d1bb-4453-be63-770efc5b1e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba3902-2b02-4c80-ac23-986069002149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
